{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT1e1Zn5WL9N"
      },
      "source": [
        "### Install dependencies\n",
        "* You're likely to be asked to restart the runtime if using colab, then do it!  \n",
        "* Ignore the warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9XRVWdoMWNdz"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install supabase\n",
        "!pip install lenskit==0.14.4\n",
        "!pip install python-dotenv\n",
        "\n",
        "!pip install numpy==1.24.3\n",
        "!pip install pandas==1.5.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HXDd8Jc6WXYa",
        "outputId": "7ed0adf5-dfd0-44df-d3d9-3b1f8f723c34"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.14.4'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Just for debug purpose\n",
        "import lenskit\n",
        "lenskit.__version__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpMiMaO8f0Jw"
      },
      "source": [
        "### Set up the secret\n",
        "Put your Supabase **url** along with **API Key** in a file, and name it `.env`  \n",
        "The file content should look like this:  \n",
        "```\n",
        "SUPABASE_KEY=<Your Supabase API Key>\n",
        "SUPABASE_URL=<Your Supabase url>\n",
        "```\n",
        "\n",
        "Then place the `.evn` file under the same directory with this notebook\n",
        "\n",
        "---\n",
        "\n",
        "#### Now go executing the next block!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z_giJ0VWa6d"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_1tvHFjWDJJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import supabase\n",
        "\n",
        "from lenskit.algorithms import Recommender\n",
        "from lenskit.algorithms.user_knn import UserUser\n",
        "from lenskit.algorithms.item_knn import ItemItem\n",
        "from lenskit.algorithms.als import BiasedMF\n",
        "from lenskit.algorithms.basic import Bias, Popular\n",
        "# from lenskit.metrics import topn\n",
        "from lenskit import batch, topn\n",
        "from lenskit.topn import RecListAnalysis\n",
        "from lenskit import crossfold as xf\n",
        "\n",
        "from typing import List, Dict, Any, Optional, Union\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76a6PvCBfaNf"
      },
      "source": [
        "### setting up customised recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0530AMMAVxNi"
      },
      "outputs": [],
      "source": [
        "class CollaborativeJobRecommender(Recommender):\n",
        "    '''\n",
        "    Collaborative filtering job recommender using LensKit\n",
        "    '''\n",
        "\n",
        "    def __init__(self, method = 'user-user', user_k = 20, item_k = 20, min_sim = 0.1, features = 20, reg = 0.1):\n",
        "        '''\n",
        "        Initialise the collaborative filtering recommender\n",
        "\n",
        "        Args:\n",
        "            method: The CF algorithm to use ('user-user', 'item-item', or 'matrix-factorisation')\n",
        "            user_k: Number of neighbours for user-based CF\n",
        "            item_k: Number of neighbours for item-based CF\n",
        "            min_sim: Minimum similarity threshold\n",
        "            features: Number of latent features for matrix factorisation\n",
        "            reg: Regularisation parameter for matrix factorisation\n",
        "        '''\n",
        "\n",
        "        self.method = method\n",
        "        self.user_k = user_k\n",
        "        self.item_k = item_k\n",
        "        self.min_sim = min_sim\n",
        "        self.features = features\n",
        "        self.reg = reg\n",
        "        self.id_col = 'job_id'\n",
        "\n",
        "        # Select algorithm based on method\n",
        "        if method == 'user-user':\n",
        "            self.algo = UserUser(nnbrs = user_k, min_sim = min_sim)\n",
        "        elif method == 'item-item':\n",
        "            self.algo = ItemItem(nnbrs = item_k, min_sim = min_sim)\n",
        "        elif method == 'matrix-factorisation':\n",
        "            self.algo = BiasedMF(features = features, reg = reg)\n",
        "        else:\n",
        "            # Default to a popularity baseline if method is not recognised\n",
        "            self.algo = Popular()\n",
        "\n",
        "        # Initialise a bias model for better predictions\n",
        "        self.bias = Bias() # finding bias of users or jobs\n",
        "\n",
        "    def fit(self, ratings):\n",
        "        '''\n",
        "        Train the recommender on the provided ratings dataframe\n",
        "\n",
        "        Args:\n",
        "            ratings: DataFrame with columns 'user', 'item', 'rating'\n",
        "        '''\n",
        "\n",
        "        # First fit the bias model\n",
        "        self.bias.fit(ratings)\n",
        "\n",
        "        # Then fit the main algorithm\n",
        "        self.algo.fit(ratings)\n",
        "\n",
        "        # Store ratings data for later use\n",
        "        self.ratings_df = ratings\n",
        "\n",
        "        # Keep track of unique users and items\n",
        "        self.users = ratings['user'].unique()\n",
        "        self.items = ratings['item'].unique()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def recommend(self, user_id, n = 10, candidates = None, ratings = None):\n",
        "        '''\n",
        "        Recommend jobs to a user based on collaborative filtering\n",
        "\n",
        "        Args:\n",
        "            user_id: ID of the user to recommend for\n",
        "            n: Number of recommendations to make\n",
        "            candidates: Optional list of job IDs to choose from\n",
        "            ratings: Optional ratings DataFrame (usually ignored in CF)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with columns 'user', 'item', 'score', 'rank'\n",
        "        '''\n",
        "\n",
        "        # Check if user exists in training data\n",
        "        if user_id not in self.users:\n",
        "            # Cold start: return popular items with bias adjustment\n",
        "            return self._handle_cold_start(user_id, n, candidates)\n",
        "\n",
        "        # Get recommendations from the algorithm\n",
        "        try:\n",
        "            # recs: recommendations\n",
        "            recs = self.algo.recommend(user_id, n, candidates)\n",
        "            if recs is not None and not recs.empty:\n",
        "                return recs\n",
        "            else:\n",
        "                return self._handle_cold_start(user_id, n, candidates)\n",
        "        except Exception as e:\n",
        "            print(f'Error getting recommendations for user {user_id}: {e}')\n",
        "            return self._handle_cold_start(user_id, n, candidates)\n",
        "\n",
        "    def _handle_cold_start(self, user_id, n = 10, candidates = None):\n",
        "        '''\n",
        "        Handle cold start problem for new users or when recommendations fail\n",
        "        '''\n",
        "        # Use the bias model to provide baseline recommendations\n",
        "\n",
        "        # if no candidate -> use all items\n",
        "        if candidates is None:\n",
        "            candidates = self.items\n",
        "\n",
        "        # Calculate bias scores for each candidate\n",
        "        scores = []\n",
        "        for item in candidates:\n",
        "            try:\n",
        "                score = self.bias.score(user_id, item)\n",
        "                scores.append({'user': user_id, 'item': item, 'score': score})\n",
        "            except:\n",
        "                # If bias scoring fails, assign a neutral score\n",
        "                scores.append({'user': user_id, 'item': item, 'score': 0.0})\n",
        "\n",
        "        # Convert to DataFrame and rank by score\n",
        "        recs_df = pd.DataFrame(scores)\n",
        "        if not recs_df.empty:\n",
        "            recs_df = recs_df.sort_values('score', ascending = False).head(n)\n",
        "            # assign ranks based on sorting result\n",
        "            recs_df['rank'] = range(1, len(recs_df) + 1)\n",
        "            return recs_df\n",
        "        else:\n",
        "            # Return empty DataFrame with correct columns if no scores are available\n",
        "            return pd.DataFrame(columns = ['user', 'item', 'score', 'rank'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhrPdjlI_oOg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare_ratings_data(jobs_df, include_applicant_data = True):\n",
        "    '''\n",
        "    Prepare ratings data from jobs DataFrame\n",
        "\n",
        "    Args:\n",
        "        jobs_df: DataFrame containing job listings with applicant data\n",
        "        include_applicant_data: Whether to include applicant data fields\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with columns 'user', 'item', 'rating'\n",
        "    '''\n",
        "\n",
        "    ratings_data = []\n",
        "\n",
        "    # Process each job to extract applicant data\n",
        "    for _, job in jobs_df.iterrows():\n",
        "        job_id = job[job_id_col]\n",
        "\n",
        "        # Extract applicant data (could be JSON string or already parsed)\n",
        "        if 'applicants' in job and pd.notna(job['applicants']):\n",
        "            try:\n",
        "                applicants = job['applicants']\n",
        "                if isinstance(applicants, str):\n",
        "                    try:\n",
        "                        applicants = json.loads(applicants)\n",
        "                    except:\n",
        "                        # Try to split by comma if JSON parsing fails\n",
        "                        if ',' in applicants:\n",
        "                            applicants = applicants.split(',')\n",
        "                        else:\n",
        "                            applicants = [applicants]\n",
        "\n",
        "                # Handle different applicant data formats\n",
        "                if isinstance(applicants, list):\n",
        "                    for applicant_id in applicants:\n",
        "                        ratings_data.append({\n",
        "                            'user': str(applicant_id).strip(),\n",
        "                            'item': job_id,\n",
        "                            'rating': 1.0  # An application is a positive signal\n",
        "                        })\n",
        "                elif isinstance(applicants, dict):\n",
        "                    for applicant_id, details in applicants.items():\n",
        "                        rating = 1.0  # Default positive rating\n",
        "\n",
        "                        # If details contain rating information, use that\n",
        "                        if isinstance(details, dict) and 'rating' in details:\n",
        "                            rating = float(details['rating'])\n",
        "\n",
        "                        ratings_data.append({\n",
        "                            'user': str(applicant_id).strip(),\n",
        "                            'item': job_id,\n",
        "                            'rating': rating\n",
        "                        })\n",
        "            except Exception as e:\n",
        "                print(f'Error processing applicants for job {job_id}: {e}')\n",
        "\n",
        "    # Additional implicit signals from applicant interaction data\n",
        "    if include_applicant_data:\n",
        "        for _, job in jobs_df.iterrows():\n",
        "            job_id = job[job_id_col]\n",
        "\n",
        "            # These fields in the job table might contain user preferences or behaviours\n",
        "            for field in ['apply_education', 'apply_skills', 'apply_major', 'apply_experience']:\n",
        "                if field in job and pd.notna(job[field]):\n",
        "                    try:\n",
        "                        data = job[field]\n",
        "                        if isinstance(data, str):\n",
        "                            try:\n",
        "                                data = json.loads(data)\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                        if isinstance(data, dict):\n",
        "                            for user_id, value in data.items():\n",
        "                                # Create weighted ratings based on field values\n",
        "                                # For example, higher skills match = higher rating\n",
        "                                weight = 0.5  # Default weight\n",
        "                                if isinstance(value, (int, float)):\n",
        "                                    weight = min(max(float(value) / 10.0, 0.1), 1.0)\n",
        "                                elif isinstance(value, dict) and 'score' in value:\n",
        "                                    weight = min(max(float(value['score']) / 10.0, 0.1), 1.0)\n",
        "\n",
        "                                ratings_data.append({\n",
        "                                    'user': str(user_id).strip(),\n",
        "                                    'item': job_id,\n",
        "                                    'rating': weight\n",
        "                                })\n",
        "                    except Exception as e:\n",
        "                        print(f'Error processing {field} for job {job_id}: {e}')\n",
        "\n",
        "    # Convert to DataFrame and handle duplicates\n",
        "    ratings_df = pd.DataFrame(ratings_data)\n",
        "    if not ratings_df.empty:\n",
        "        # Aggregate duplicate user-item pairs by taking the maximum rating\n",
        "        ratings_df = ratings_df.groupby(['user', 'item'])['rating'].max().reset_index()\n",
        "\n",
        "    return ratings_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVNbwX1k1Gmz"
      },
      "source": [
        "### Format job information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DBleslt_vBp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def format_job_info(job):\n",
        "    '''Format job information for display'''\n",
        "\n",
        "    job_title = job.get('job_title', job.get('job_name', 'Untitled Job'))\n",
        "    company = job.get('company_name', 'Unknown Company')\n",
        "    similarity = job.get('score', 0)\n",
        "\n",
        "    info = f\"- {job_title} at {company} (ID: {job['job_id']})\"\n",
        "    info += f'\\n  Recommendation Score: {similarity:.4f}'\n",
        "\n",
        "    # Add more relevant job info\n",
        "    if 'job_industry' in job and pd.notna(job['job_industry']):\n",
        "        info += f\"\\n  Industry: {job['job_industry']}\"\n",
        "\n",
        "    if 'location' in job and pd.notna(job['location']):\n",
        "        info += f\"\\n  Location: {job['location']}\"\n",
        "\n",
        "    if 'legal_benefits' in job and pd.notna(job['legal_benefits']):\n",
        "        info += f\"\\n  Benefits: {job['legal_benefits']}\"\n",
        "\n",
        "    return info\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yplPFPsI0-p5"
      },
      "source": [
        "### Recommender evaluation (optional)\n",
        "<!-- [NDCG explanation](https://hackmd.io/@tsungjung411/H1Eu4isLyx) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UieKryBVAhZm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_recommender(jobs_df, ratings_df, method = 'user-user'):\n",
        "    '''\n",
        "    Evaluate the recommender using cross-validation\n",
        "\n",
        "    Args:\n",
        "        jobs_df: DataFrame containing job data\n",
        "        ratings_df: DataFrame with user-item interactions\n",
        "        method: Collaborative filtering method to evaluate\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with evaluation metrics\n",
        "    '''\n",
        "    # Define algorithms to evaluate\n",
        "    algorithms = {\n",
        "        'Popular': Popular(),\n",
        "        'BiasModel': Bias(),\n",
        "        method: CollaborativeJobRecommender(method = method)\n",
        "    }\n",
        "\n",
        "    # Set up evaluation\n",
        "    eval_results = []\n",
        "\n",
        "    # Create 5-fold cross-validation splits\n",
        "    for train, test in xf.partition_users( ratings_df, 5, xf.SampleFrac(0.2) ):\n",
        "        # Clone the jobs_df to avoid modifying the original\n",
        "        train_jobs = jobs_df.copy()\n",
        "\n",
        "        # Create 'candidates' set - all items in the test set\n",
        "        candidates = test['item'].unique()\n",
        "\n",
        "        # Create a function to filter candidates\n",
        "        def candidates_func(user):\n",
        "            return candidates\n",
        "\n",
        "        # For each algorithm\n",
        "        for name, algo in algorithms.items():\n",
        "            # Train the algorithm\n",
        "            algo.fit(train)\n",
        "\n",
        "            # Generate recommendations for test users\n",
        "            # Use the candidates_func for filtering\n",
        "            recs = batch.recommend(algo, test['user'].unique(), 10, candidates = candidates_func)\n",
        "\n",
        "            # Recommendation List Analysis (RLA)\n",
        "            rla = topn.RecListAnalysis()\n",
        "            rla.add_metric(topn.ndcg)  # Example: Add ndcg metric\n",
        "            # You can add other metrics like precision, recall, etc.\n",
        "            _metrics = rla.compute(recs, test)\n",
        "\n",
        "            # Compute evaluation metrics\n",
        "            # _metrics = topn.compute_metrics(test, recs, include_missing = True)\n",
        "\n",
        "            # choose certain algorithm\n",
        "            _metrics['Algorithm'] = name\n",
        "            eval_results.append(_metrics)\n",
        "\n",
        "    # Combine and return all results\n",
        "    return pd.concat(eval_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtxvTA5dAkYc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main():\n",
        "    '''\n",
        "    Main function to run the collaborative filtering recommender\n",
        "    '''\n",
        "\n",
        "    print('Initialising collaborative job recommender system...')\n",
        "\n",
        "    # Load environment variables\n",
        "    load_dotenv()\n",
        "\n",
        "    # Retrieve the url and API key from environment\n",
        "    supabase_url = os.environ.get('SUPABASE_URL')\n",
        "    supabase_key = os.environ.get('SUPABASE_KEY')\n",
        "\n",
        "    # Check whether the setup goes smoothly\n",
        "    if supabase_key:\n",
        "        print('Supabase key loaded successfully.')\n",
        "    else:\n",
        "        print(\"Supabase key not found. Please make sure it's defined as a secret.\")\n",
        "        return\n",
        "\n",
        "    # Connect to supabase\n",
        "    supabase_client = supabase.create_client(supabase_url, supabase_key)\n",
        "\n",
        "    # Get all jobs from supabase\n",
        "    jobs_data = supabase_client.from_('jobs').select('*').execute().data\n",
        "\n",
        "    # Load the data into a DataFrame\n",
        "    jobs_df = pd.DataFrame(jobs_data)\n",
        "\n",
        "    # in case there is nothing\n",
        "    if jobs_df.empty:\n",
        "        print('Error: No job data was loaded from Supabase.')\n",
        "        return\n",
        "\n",
        "    print(f'Loaded {len(jobs_df)} jobs from database.')\n",
        "\n",
        "    # Define global ID column name\n",
        "    global job_id_col\n",
        "    job_id_col = 'job_id'\n",
        "\n",
        "    # Prepare ratings data from job applications\n",
        "    print('Preparing ratings data from job applications...')\n",
        "    ratings_df = prepare_ratings_data(jobs_df)\n",
        "\n",
        "    # in case rating is missed\n",
        "    if ratings_df.empty:\n",
        "        print('Error: No ratings data could be extracted.')\n",
        "        return\n",
        "\n",
        "    print(f'Created {len(ratings_df)} user-job interactions.')\n",
        "\n",
        "    # Initialise and train the recommender\n",
        "    print('Training collaborative filtering recommenders...')\n",
        "\n",
        "    # Create three different recommenders for comparison\n",
        "    user_user_rec = CollaborativeJobRecommender(method = 'user-user')\n",
        "    item_item_rec = CollaborativeJobRecommender(method = 'item-item')\n",
        "    mf_rec = CollaborativeJobRecommender(method = 'matrix-factorisation')\n",
        "\n",
        "    # Train all recommenders\n",
        "    user_user_rec.fit(ratings_df)\n",
        "    item_item_rec.fit(ratings_df)\n",
        "    mf_rec.fit(ratings_df)\n",
        "\n",
        "    print('Recommender systems trained successfully.')\n",
        "\n",
        "    # Example 1: Find users with most applications for testing\n",
        "    user_counts = ratings_df['user'].value_counts()\n",
        "    test_user_id = user_counts.index[0]  # User with most applications\n",
        "    test_count = user_counts.iloc[0]\n",
        "    print(f'\\nTest user {test_user_id} has applied to {test_count} jobs')\n",
        "\n",
        "    # Get recommendations from each method\n",
        "    for name, rec in [('User-User CF', user_user_rec),\n",
        "                      ('Item-Item CF', item_item_rec),\n",
        "                      ('Matrix Factorisation', mf_rec)]:\n",
        "        print(f'\\nGenerating recommendations using {name}...')\n",
        "        # Get recommendations for the test user (top n)\n",
        "        recommendations = rec.recommend(test_user_id, n = 3)\n",
        "\n",
        "        if recommendations.empty:\n",
        "            print(f'No recommendations found for user {test_user_id} using {name}.')\n",
        "        else:\n",
        "            print(f'Top 3 job recommendations for user {test_user_id} using {name}:')\n",
        "            for _, rec_row in recommendations.iterrows():\n",
        "                job_id = rec_row['item']\n",
        "                job = jobs_df[jobs_df[job_id_col] == job_id]\n",
        "\n",
        "                if not job.empty:\n",
        "                    job_dict = job.iloc[0].to_dict()\n",
        "                    job_dict['score'] = rec_row['score']\n",
        "                    print(format_job_info(job_dict) + '\\n')\n",
        "\n",
        "    # Example 2: Create a test case for a new user with one job application\n",
        "    print('\\nSimulating a new user applying to a job...')\n",
        "    new_user_id = 'new_test_user_789'\n",
        "\n",
        "    # Pick a random job for the test\n",
        "    test_job_id = jobs_df.iloc[5][job_id_col]  # A different job than first example\n",
        "\n",
        "    print(f'New user {new_user_id} applied to job {test_job_id}')\n",
        "\n",
        "    # Create a test ratings DataFrame for this new user\n",
        "    test_ratings = pd.DataFrame({\n",
        "        'user': [new_user_id],\n",
        "        'item': [test_job_id],\n",
        "        'rating': [1.0]\n",
        "    })\n",
        "\n",
        "    # Combine with existing ratings\n",
        "    combined_ratings = pd.concat([ratings_df, test_ratings])\n",
        "\n",
        "    # Train a new recommender with the combined data\n",
        "    test_rec = CollaborativeJobRecommender(method = 'item-item')\n",
        "    test_rec.fit(combined_ratings)\n",
        "\n",
        "    # Get recommendations for the new user\n",
        "    recommendations = test_rec.recommend(new_user_id, n = 3)\n",
        "\n",
        "    if recommendations.empty:\n",
        "        print('No recommendations found for the new user.')\n",
        "    else:\n",
        "        print(f'Top 3 job recommendations for the new user:')\n",
        "        for _, rec_row in recommendations.iterrows():\n",
        "            job_id = rec_row['item']\n",
        "            job = jobs_df[jobs_df[job_id_col] == job_id]\n",
        "\n",
        "            if not job.empty:\n",
        "                job_dict = job.iloc[0].to_dict()\n",
        "                job_dict['score'] = rec_row['score']\n",
        "                print(format_job_info(job_dict) + '\\n')\n",
        "\n",
        "    # Optionally run evaluation\n",
        "    print('\\nWould you like to run evaluation? This may take some time. (y/n)')\n",
        "    response = input()\n",
        "    if response.lower() == 'y':\n",
        "        print('Running recommender evaluation...')\n",
        "        eval_results = evaluate_recommender(jobs_df, ratings_df, method = 'user-user')\n",
        "        print('\\nEvaluation Results:')\n",
        "        print(eval_results.groupby('Algorithm').mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSimwRjTAncp",
        "outputId": "681a302d-c969-4667-8bcb-723e0ef82aa2"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
